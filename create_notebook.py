
import json

notebook = {
    "cells": [
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch>=2.0 transformers>=4.30 scikit-learn>=1.3 pandas>=1.5 numpy>=1.23 pytorch-lightning>=2.0 pyarrow>=12.0 python-dotenv>=1.0 tqdm>=4.65 pytest>=7.4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import sys\n",
                "from typing import Any, Callable, List, Tuple, Dict\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import pytorch_lightning as pl\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from itertools import combinations\n",
                "from tqdm import tqdm\n",
                "from transformers import pipeline, AutoTokenizer\n",
                "import json\n",
                "import pickle\n",
                "import random\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# File paths for input CSV files\n",
                "DATA_DIR = \"/kaggle/input/yelp-data\"\n",
                "INPUT_FILES = {\n",
                "    \"business\": os.path.join(DATA_DIR, \"yelp_business_data.csv\"),\n",
                "    \"review\": os.path.join(DATA_DIR, \"yelp_review.csv\"),\n",
                "    \"user\": os.path.join(DATA_DIR, \"yelp_user.csv\"),\n",
                "    \"checkin\": os.path.join(DATA_DIR, \"yelp_checkin_data.csv\"),\n",
                "    \"tip\": os.path.join(DATA_DIR, \"yelp_tip_data.csv\")\n",
                "}\n",
                "\n",
                "# Output paths for processed data\n",
                "OUTPUT_DIR = os.path.join(\"data\", \"processed\")\n",
                "OUTPUT_FILES = {\n",
                "    \"merged_data\": os.path.join(OUTPUT_DIR, \"merged_data.csv\"),\n",
                "    \"featured_data\": os.path.join(OUTPUT_DIR, \"featured_data.csv\"),\n",
                "    \"sentiment_data\": os.path.join(OUTPUT_DIR, \"sentiment_data.csv\"),\n",
                "    \"final_model_data\": os.path.join(OUTPUT_DIR, \"final_model_data.csv\")\n",
                "}\n",
                "\n",
                "FEATURED_DATA_PATH = OUTPUT_FILES[\"featured_data\"]\n",
                "\n",
                "# Model hyperparameters\n",
                "LEARNING_RATE = 0.0001\n",
                "BATCH_SIZE = 64\n",
                "MAX_EPOCHS = 40\n",
                "\n",
                "# Feature lists\n",
                "CANDIDATE_FEATURES = [\n",
                "    \"user_average_stars\",\n",
                "    \"business_average_stars\",\n",
                "    \"user_review_count\",\n",
                "    \"business_review_count\",\n",
                "    \"time_yelping\",\n",
                "    \"date_year\",\n",
                "    \"total_elite_statuses\",\n",
                "    \"elite_status\",\n",
                "    \"normalized_sentiment_score\"\n",
                "]\n",
                "\n",
                "EXPECTED_OPTIMAL_FEATURES = [\n",
                "    \"user_average_stars\",\n",
                "    \"business_average_stars\",\n",
                "    \"time_yelping\",\n",
                "    \"elite_status\",\n",
                "    \"normalized_sentiment_score\"\n",
                "]\n",
                "\n",
                "# Random seed\n",
                "SEED = 1\n",
                "\n",
                "# Sentiment settings\n",
                "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "MAX_TOKENS = 512\n",
                "SENTIMENT_BATCH_SIZE = 64\n",
                "\n",
                "# Device detection helper function\n",
                "def get_device():\n",
                "    \"\"\"\n",
                "    Detect the available device for PyTorch computations.\n",
                "\n",
                "    Returns:\n",
                "        str: 'mps' if MPS is available, 'cuda' if CUDA is available, otherwise 'cpu'\n",
                "    \"\"\"\n",
                "    if torch.backends.mps.is_available():\n",
                "        return \"mps\"\n",
                "    elif torch.cuda.is_available():\n",
                "        return \"cuda\"\n",
                "    else:\n",
                "        return \"cpu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_elite_years(elite_str: str) -> List[int]:\n",
                "    \"\"\"\n",
                "    Parse elite years string into a list of integers.\n",
                "\n",
                "    Handles empty strings, NaN values, and comma/pipe-separated years.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years, e.g., \"2018,2019,2020\" or \"2018|2019\"\n",
                "\n",
                "    Returns:\n",
                "        List of integers representing elite years, or empty list for invalid input\n",
                "    \"\"\"\n",
                "    if pd.isna(elite_str) or elite_str == \"\":\n",
                "        return []\n",
                "\n",
                "    # Replace pipe with comma for consistent splitting\n",
                "    elite_str = elite_str.replace('|', ',')\n",
                "\n",
                "    # Split by comma and convert to integers, filtering out empty strings\n",
                "    years = []\n",
                "    for year_str in elite_str.split(','):\n",
                "        year_str = year_str.strip()\n",
                "        if year_str:\n",
                "            try:\n",
                "                years.append(int(year_str))\n",
                "            except ValueError:\n",
                "                # Skip invalid year strings\n",
                "                continue\n",
                "\n",
                "    return years\n",
                "\n",
                "def count_elite_statuses(elite_str: str, review_year: int) -> int:\n",
                "    \"\"\"\n",
                "    Count the number of elite statuses up to and including the review year.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years\n",
                "        review_year: The year of the review\n",
                "\n",
                "    Returns:\n",
                "        Number of elite years <= review_year\n",
                "    \"\"\"\n",
                "    elite_years = parse_elite_years(elite_str)\n",
                "    return sum(1 for year in elite_years if year <= review_year)\n",
                "\n",
                "def check_elite_status(elite_str: str, review_year: int) -> int:\n",
                "    \"\"\"\n",
                "    Check if the user was elite in the review year or the previous year.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years\n",
                "        review_year: The year of the review\n",
                "\n",
                "    Returns:\n",
                "        1 if elite in review_year or (review_year - 1), 0 otherwise\n",
                "    \"\"\"\n",
                "    elite_years = parse_elite_years(elite_str)\n",
                "    return 1 if review_year in elite_years or (review_year - 1) in elite_years else 0\n",
                "\n",
                "def smart_truncate_text(text: str, tokenizer, max_tokens: int = 500) -> str:\n",
                "    \"\"\"\n",
                "    Tokenize text, keep first 250 + last 250 tokens if over max_tokens, convert back to string.\n",
                "    \"\"\"\n",
                "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
                "    if len(tokens) <= max_tokens:\n",
                "        return text\n",
                "    # Keep first 250 and last 250\n",
                "    first_part = tokens[:250]\n",
                "    last_part = tokens[-250:]\n",
                "    truncated_tokens = first_part + last_part\n",
                "    return tokenizer.decode(truncated_tokens)\n",
                "\n",
                "def set_seed(seed: int = 1) -> None:\n",
                "    \"\"\"\n",
                "    Set random seed for reproducibility.\n",
                "    \"\"\"\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.backends.mps.is_available():\n",
                "        torch.mps.manual_seed(seed)\n",
                "\n",
                "def verify_gpu_support() -> bool:\n",
                "    \"\"\"\n",
                "    Check MPS GPU support availability.\n",
                "    \"\"\"\n",
                "    available = torch.backends.mps.is_available()\n",
                "    status = \"available\" if available else \"not available\"\n",
                "    logger.info(f\"MPS GPU support is {status}.\")\n",
                "    return available"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_business_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load business data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the business CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing business data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Business data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for business data\n",
                "    dtypes = {\n",
                "        'business_id': str,\n",
                "        'name': str,\n",
                "        'address': str,\n",
                "        'city': str,\n",
                "        'state': str,\n",
                "        'postal_code': str,\n",
                "        'latitude': float,\n",
                "        'longitude': float,\n",
                "        'stars': float,\n",
                "        'review_count': int,\n",
                "        'is_open': int\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['business_id', 'name', 'stars', 'review_count']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in business data: {missing_columns}\")\n",
                "\n",
                "    return df\n",
                "\n",
                "def load_review_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load review data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the review CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing review data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Review data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for review data\n",
                "    dtypes = {\n",
                "        'review_id': str,\n",
                "        'user_id': str,\n",
                "        'business_id': str,\n",
                "        'stars': int,\n",
                "        'useful': int,\n",
                "        'funny': int,\n",
                "        'cool': int,\n",
                "        'text': str,\n",
                "        'date': str\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in review data: {missing_columns}\")\n",
                "\n",
                "    return df\n",
                "\n",
                "def load_user_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load user data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the user CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing user data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"User data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for user data\n",
                "    dtypes = {\n",
                "        'user_id': str,\n",
                "        'name': str,\n",
                "        'review_count': int,\n",
                "        'yelping_since': str,\n",
                "        'useful': int,\n",
                "        'funny': int,\n",
                "        'cool': int,\n",
                "        'elite': str,\n",
                "        'friends': str,\n",
                "        'fans': int,\n",
                "        'average_stars': float,\n",
                "        'compliment_hot': int,\n",
                "        'compliment_more': int,\n",
                "        'compliment_profile': int,\n",
                "        'compliment_cute': int,\n",
                "        'compliment_list': int,\n",
                "        'compliment_note': int,\n",
                "        'compliment_plain': int,\n",
                "        'compliment_cool': int,\n",
                "        'compliment_funny': int,\n",
                "        'compliment_writer': int,\n",
                "        'compliment_photos': int\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['user_id', 'name', 'review_count', 'yelping_since', 'average_stars']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in user data: {missing_columns}\")\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rename_columns(user_df: pd.DataFrame, business_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    \"\"\"\n",
                "    Rename columns in user and business DataFrames to avoid naming conflicts.\n",
                "\n",
                "    Args:\n",
                "        user_df: DataFrame containing user data\n",
                "        business_df: DataFrame containing business data\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (renamed_user_df, renamed_business_df)\n",
                "    \"\"\"\n",
                "    # Rename user columns\n",
                "    user_renames = {\n",
                "        'useful': 'total_useful',\n",
                "        'funny': 'total_funny',\n",
                "        'cool': 'total_cool',\n",
                "        'review_count': 'user_review_count',\n",
                "        'name': 'user_name',\n",
                "        'average_stars': 'user_average_stars'\n",
                "    }\n",
                "\n",
                "    # Rename business columns\n",
                "    business_renames = {\n",
                "        'stars': 'business_average_stars',\n",
                "        'review_count': 'business_review_count',\n",
                "        'name': 'business_name'\n",
                "    }\n",
                "\n",
                "    # Apply renames\n",
                "    renamed_user_df = user_df.rename(columns=user_renames)\n",
                "    renamed_business_df = business_df.rename(columns=business_renames)\n",
                "\n",
                "    return renamed_user_df, renamed_business_df\n",
                "\n",
                "def convert_date_columns(review_df: pd.DataFrame, user_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    \"\"\"\n",
                "    Convert date columns to datetime dtype.\n",
                "\n",
                "    Args:\n",
                "        review_df: DataFrame containing review data\n",
                "        user_df: DataFrame containing user data\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (converted_review_df, converted_user_df)\n",
                "    \"\"\"\n",
                "    # Convert 'date' column in review_df to datetime\n",
                "    converted_review_df = review_df.copy()\n",
                "    converted_review_df['date'] = pd.to_datetime(converted_review_df['date'])\n",
                "\n",
                "    # Convert 'yelping_since' column in user_df to datetime\n",
                "    converted_user_df = user_df.copy()\n",
                "    converted_user_df['yelping_since'] = pd.to_datetime(converted_user_df['yelping_since'])\n",
                "\n",
                "    return converted_review_df, converted_user_df\n",
                "\n",
                "def merge_datasets(review_df: pd.DataFrame, user_df: pd.DataFrame, business_df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Merge review, user, and business DataFrames using inner joins.\n",
                "\n",
                "    Args:\n",
                "        review_df: DataFrame containing review data\n",
                "        user_df: DataFrame containing user data\n",
                "        business_df: DataFrame containing business data\n",
                "\n",
                "    Returns:\n",
                "        Merged DataFrame with all three sources combined\n",
                "    \"\"\"\n",
                "    # Inner join review -> user on 'user_id'\n",
                "    merged = review_df.merge(user_df, on='user_id', how='inner')\n",
                "    # Then result -> business on 'business_id'\n",
                "    merged = merged.merge(business_df, on='business_id', how='inner')\n",
                "    return merged\n",
                "\n",
                "def clean_merged_data(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Clean merged DataFrame by removing rows with missing values in critical columns.\n",
                "\n",
                "    Args:\n",
                "        merged_df: Merged DataFrame from merge_datasets\n",
                "\n",
                "    Returns:\n",
                "        Cleaned DataFrame with no missing values in critical columns\n",
                "    \"\"\"\n",
                "    # Drop rows with missing values in specified columns\n",
                "    cleaned = merged_df.dropna(subset=['stars', 'text', 'business_average_stars', 'user_average_stars', 'user_review_count'])\n",
                "    return cleaned\n",
                "\n",
                "def preprocess_pipeline() -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete preprocessing pipeline: load, rename, convert dates, merge, clean, and save.\n",
                "\n",
                "    Returns:\n",
                "        Final preprocessed DataFrame\n",
                "    \"\"\"\n",
                "    # Load all three datasets\n",
                "    review_df = load_review_data(INPUT_FILES[\"review\"])\n",
                "    user_df = load_user_data(INPUT_FILES[\"user\"])\n",
                "    business_df = load_business_data(INPUT_FILES[\"business\"])\n",
                "\n",
                "    # Rename columns\n",
                "    user_df, business_df = rename_columns(user_df, business_df)\n",
                "\n",
                "    # Convert date columns\n",
                "    review_df, user_df = convert_date_columns(review_df, user_df)\n",
                "\n",
                "    # Merge datasets\n",
                "    merged_df = merge_datasets(review_df, user_df, business_df)\n",
                "\n",
                "    # Clean merged data\n",
    "    cleaned_df = clean_merged_data(merged_df)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(OUTPUT_FILES[\"merged_data\"])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    cleaned_df.to_csv(OUTPUT_FILES[\"merged_data\"], index=False)\n",
    "\n",
    "    return cleaned_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def engineer_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Engineer time-based features from the DataFrame.\n",
                "\n",
                "    Calculates time_yelping as the difference between date and yelping_since\n",
                "    in weeks, and extracts date_year from the date column.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with 'date' and 'yelping_since' columns\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added 'time_yelping' and 'date_year' columns\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    # Convert date columns to datetime if they are strings\n",
                "    df['date'] = pd.to_datetime(df['date'])\n",
                "    df['yelping_since'] = pd.to_datetime(df['yelping_since'])\n",
                "\n",
                "    # Calculate time_yelping in weeks\n",
                "    df['time_yelping'] = (df['date'] - df['yelping_since']).dt.total_seconds() / (7 * 24 * 3600)\n",
                "\n",
                "    # Extract year from date\n",
                "    df['date_year'] = df['date'].dt.year\n",
                "\n",
                "    return df\n",
                "\n",
                "def engineer_elite_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Engineer elite status features from the DataFrame.\n",
                "\n",
                "    Creates 'total_elite_statuses' by counting elite years up to the review year,\n",
                "    and 'elite_status' by checking if the user was elite in the review year or previous year.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with 'elite' and 'date_year' columns\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added 'total_elite_statuses' and 'elite_status' columns\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    # Create total_elite_statuses using count_elite_statuses\n",
                "    df['total_elite_statuses'] = df.apply(\n",
                "        lambda row: count_elite_statuses(row['elite'], row['date_year']),\n",
                "        axis=1\n",
                "    )\n",
                "\n",
                "    # Create elite_status using check_elite_status\n",
                "    df['elite_status'] = df.apply(\n",
                "        lambda row: check_elite_status(row['elite'], row['date_year']),\n",
                "        axis=1\n",
                "    )\n",
                "\n",
                "    return df\n",
                "\n",
                "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Handle missing values in the DataFrame.\n",
                "\n",
                "    Fills 'time_yelping' with the median value, and 'total_elite_statuses'\n",
                "    and 'elite_status' with 0.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with missing values handled\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    df['time_yelping'] = df['time_yelping'].fillna(df['time_yelping'].median())\n",
                "    df['total_elite_statuses'] = df['total_elite_statuses'].fillna(0)\n",
                "    df['elite_status'] = df['elite_status'].fillna(0)\n",
                "\n",
                "    return df\n",
                "\n",
                "def feature_engineering_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete feature engineering pipeline.\n",
                "\n",
                "    Applies time feature engineering, elite feature engineering, handles missing values,\n",
                "    saves the processed data to CSV, and returns the DataFrame.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame\n",
                "\n",
                "    Returns:\n",
                "        Processed DataFrame with engineered features\n",
                "    \"\"\"\n",
                "    df = engineer_time_features(df)\n",
                "    df = engineer_elite_features(df)\n",
                "    df = handle_missing_values(df)\n",
                "\n",
                "    df.to_csv(FEATURED_DATA_PATH, index=False)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def initialize_sentiment_pipeline(device: str = \"cpu\"):\n",
                "    \"\"\"\n",
                "    Initialize sentiment analysis pipeline with device detection.\n",
                "\n",
                "    Args:\n",
                "        device: Device to use ('mps' or 'cpu'). Defaults to 'cpu' for Kaggle compatibility.\n",
                "\n",
                "    Returns:\n",
                "        Hugging Face pipeline for sentiment analysis\n",
                "    \"\"\"\n",
                "    # For Kaggle, use CPU as default\n",
                "    if device not in [\"cpu\", \"cuda\"]:\n",
                "        device = \"cpu\"\n",
                "\n",
                "    # Initialize pipeline\n",
                "    sentiment_pipeline = pipeline(\n",
                "        \"sentiment-analysis\",\n",
                "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "        device=device,\n",
                "        truncation=False\n",
                "    )\n",
                "\n",
                "    return sentiment_pipeline\n",
                "\n",
                "def process_sentiment_batch(texts: List[str], pipeline, batch_size: int = 64) -> List[Dict]:\n",
                "    \"\"\"\n",
                "    Process batch of texts through sentiment analysis pipeline.\n",
                "\n",
                "    Args:\n",
                "        texts: List of text strings to analyze\n",
                "        pipeline: Hugging Face sentiment analysis pipeline\n",
                "        batch_size: Number of texts to process in each batch\n",
                "\n",
                "    Returns:\n",
                "        List of sentiment analysis results (dicts with 'label' and 'score')\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    for i in range(0, len(texts), batch_size):\n",
                "        batch = texts[i:i + batch_size]\n",
                "        batch_results = pipeline(batch)\n",
                "        results.extend(batch_results)\n",
                "    return results\n",
                "\n",
                "def normalize_sentiment_scores(sentiment_results: List[Dict]) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Normalize sentiment scores to range [-1, 1].\n",
                "\n",
                "    Args:\n",
                "        sentiment_results: List of sentiment analysis results\n",
                "\n",
                "    Returns:\n",
                "        Pandas Series with normalized scores (-1 for negative, +1 for positive)\n",
                "    \"\"\"\n",
                "    scores = []\n",
                "    for result in sentiment_results:\n",
                "        label = result['label']\n",
                "        score = result['score']\n",
                "        if label == 'NEGATIVE':\n",
                "            scores.append(-score)\n",
                "        elif label == 'POSITIVE':\n",
                "            scores.append(score)\n",
                "        else:\n",
                "            # Handle unexpected labels (e.g., neutral) by setting to 0\n",
                "            scores.append(0.0)\n",
                "    return pd.Series(scores)\n",
                "\n",
                "def sentiment_analysis_pipeline(df: pd.DataFrame, batch_size: int = 64) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete sentiment analysis pipeline.\n",
                "\n",
                "    Args:\n",
                "        df: DataFrame containing review texts in 'text' column\n",
                "        batch_size: Number of texts to process in each batch\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added sentiment columns\n",
                "    \"\"\"\n",
                "    # Initialize sentiment pipeline\n",
                "    sentiment_pipeline = initialize_sentiment_pipeline()\n",
                "\n",
                "    # Load tokenizer for truncation\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "    # Process in batches with tqdm progress bar\n",
                "    results = []\n",
                "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing sentiment analysis\"):\n",
                "        batch_texts = df['text'].iloc[i:i + batch_size].tolist()\n",
                "        # Apply smart truncation to each text\n",
                "        truncated_texts = [smart_truncate_text(text, tokenizer, max_tokens=500) for text in batch_texts]\n",
                "        batch_results = process_sentiment_batch(truncated_texts, sentiment_pipeline, batch_size)\n",
                "        results.extend(batch_results)\n",
                "\n",
                "    # Normalize sentiment scores\n",
                "    normalized_scores = normalize_sentiment_scores(results)\n",
                "\n",
                "    # Add columns\n",
                "    df['sentiment_label'] = [r['label'] for r in results]\n",
                "    df['sentiment_score_raw'] = [r['score'] for r in results]\n",
                "    df['normalized_sentiment_score'] = normalized_scores\n",
                "\n",
                "    # Save to CSV\n",
                "    output_path = OUTPUT_FILES[\"sentiment_data\"]\n",
                "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "    df.to_csv(output_path, index=False)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_feature_data(df: pd.DataFrame, candidate_features: List[str]) -> Tuple[pd.DataFrame, pd.Series]:\n",
                "    \"\"\"\n",
                "    Prepare data for feature selection.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame containing all features and target\n",
                "        candidate_features: List of feature column names to consider\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (X, y) where X is features DataFrame and y is target Series\n",
                "    \"\"\"\n",
                "    # Select candidate features plus target column\n",
                "    selected_cols = candidate_features + ['stars']\n",
                "    subset_df = df[selected_cols].copy()\n",
                "\n",
                "    # Remove rows with missing values in selected columns\n",
                "    subset_df = subset_df.dropna()\n",
                "\n",
                "    # Separate features and target\n",
                "    X = subset_df[candidate_features]\n",
                "    y = subset_df['stars']\n",
                "\n",
                "    return X, y\n",
                "\n",
                "def run_best_subset_selection(X: pd.DataFrame, y: pd.Series) -> List[str]:\n",
                "    \"\"\"\n",
                "    Run best subset feature selection using exhaustive search over different numbers of features.\n",
                "\n",
                "    Args:\n",
                "        X: Feature DataFrame\n",
                "        y: Target Series\n",
                "\n",
                "    Returns:\n",
                "        List of selected feature names\n",
                "    \"\"\"\n",
                "    # Split data into train and validation\n",
                "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "    best_overall_mse = float('inf')\n",
                "    best_k = None\n",
                "    best_features = None\n",
                "\n",
                "    max_k = min(10, len(X.columns))\n",
                "\n",
                "    for k in tqdm(range(1, max_k + 1), desc=\"Evaluating k\"):\n",
                "        # Get all combinations of k features\n",
                "        feature_combos = list(combinations(X.columns, k))\n",
                "\n",
                "        best_mse_for_k = float('inf')\n",
                "        best_combo_for_k = None\n",
                "\n",
                "        for combo in tqdm(feature_combos, desc=f\"Evaluating combinations for k={k}\"):\n",
                "            combo = list(combo)\n",
                "            # Select features\n",
                "            X_train_combo = X_train[combo]\n",
                "            X_val_combo = X_val[combo]\n",
                "\n",
                "            # Fit model\n",
                "            model = RandomForestRegressor(n_estimators=100, random_state=1, n_jobs=-1)\n",
                "            model.fit(X_train_combo, y_train)\n",
                "\n",
                "            # Predict and compute MSE\n",
                "            y_pred = model.predict(X_val_combo)\n",
                "            mse = mean_squared_error(y_val, y_pred)\n",
                "\n",
                "            if mse < best_mse_for_k:\n",
                "                best_mse_for_k = mse\n",
                "                best_combo_for_k = combo\n",
                "\n",
                "        # Compare across k\n",
                "        if best_mse_for_k < best_overall_mse:\n",
                "            best_overall_mse = best_mse_for_k\n",
                "            best_k = k\n",
                "            best_features = best_combo_for_k\n",
                "\n",
                "    print(f\"Best k: {best_k}\")\n",
                "    print(f\"Best feature set: {best_features}\")\n",
                "    print(f\"Best MSE: {best_overall_mse}\")\n",
                "\n",
                "    return best_features\n",
                "\n",
                "def feature_selection_pipeline(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
                "    \"\"\"\n",
                "    Complete feature selection pipeline.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with all features and target\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (final DataFrame with optimal features + target, list of optimal features)\n",
                "    \"\"\"\n",
                "    # Get candidate features from config\n",
                "    candidate_features = CANDIDATE_FEATURES\n",
                "\n",
                "    # Prepare feature data\n",
                "    X, y = prepare_feature_data(df, candidate_features)\n",
                "\n",
                "    # Run best subset selection\n",
                "    optimal_features = run_best_subset_selection(X, y)\n",
                "\n",
                "    # Save optimal features to JSON\n",
                "    optimal_features_path = os.path.join(OUTPUT_DIR, \"optimal_features.json\")\n",
                "    with open(optimal_features_path, 'w') as f:\n",
                "        json.dump(optimal_features, f, indent=2)\n",
                "\n",
                "    # Create final dataset with optimal features + target\n",
                "    final_cols = optimal_features + ['stars']\n",
                "    final_df = df[final_cols].copy()\n",
                "\n",
                "    # Remove any remaining missing values\n",
                "    final_df = final_df.dropna()\n",
                "\n",
                "    # Save final dataset\n",
    "    final_data_path = OUTPUT_FILES[\"final_model_data\"]\n",
    "    os.makedirs(os.path.dirname(final_data_path), exist_ok=True)\n",
    "    final_df.to_csv(final_data_path, index=False)\n",
    "\n",
    "    # Return final DataFrame and feature list\n",
    "    return final_df, optimal_features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class YelpRatingPredictor(pl.LightningModule):\n",
                "    \"\"\"\n",
                "    PyTorch Lightning module for predicting Yelp ratings using a neural network.\n",
                "\n",
                "    This model consists of a feedforward neural network with dropout and batch normalization\n",
                "    layers, designed to predict star ratings based on input features.\n",
                "\n",
                "    Attributes:\n",
                "        network: Sequential neural network layers\n",
                "        criterion: Mean squared error loss function\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, input_size: int = 5, learning_rate: float = 0.0001) -> None:\n",
                "        super().__init__()\n",
                "        self.network = nn.Sequential(\n",
                "            nn.Linear(input_size, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "        self.criterion = nn.MSELoss()\n",
                "        self.save_hyperparameters()\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the network.\n",
                "\n",
                "        Args:\n",
                "            x: Input tensor\n",
                "\n",
                "        Returns:\n",
                "            Output tensor predictions\n",
                "        \"\"\"\n",
                "        return self.network(x)\n",
                "\n",
                "    def training_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Training step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Loss tensor\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        self.log('train_loss', loss)\n",
                "        return loss\n",
                "\n",
                "    def validation_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Validation step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Loss tensor\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        mae = torch.mean(torch.abs(preds - y))\n",
                "        self.log('val_loss', loss)\n",
                "        self.log('val_mae', mae)\n",
                "        return loss\n",
                "\n",
                "    def test_step(self, batch: tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\n",
                "        \"\"\"\n",
                "        Test step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Dictionary with test loss and MAE\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        mae = torch.mean(torch.abs(preds - y))\n",
                "        return {'test_loss': loss, 'test_mae': mae}\n",
                "\n",
                "    def configure_optimizers(self) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Configure optimizer and learning rate scheduler.\n",
                "\n",
                "        Returns:\n",
                "            Dictionary with optimizer and scheduler configuration\n",
                "        \"\"\"\n",
                "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams.learning_rate)\n",
                "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "            optimizer, factor=0.5, patience=3\n",
                "        )\n",
                "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stratify_and_split(df: pd.DataFrame, target_size: int = 130000) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stratify the dataset by 'stars' and downsample to equal samples per class.\n",
                "\n",
