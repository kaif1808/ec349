
import json

notebook = {
    "cells": [
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch>=2.0 transformers>=4.30 scikit-learn>=1.3 pandas>=1.5 numpy>=1.23 pytorch-lightning>=2.0 pyarrow>=12.0 python-dotenv>=1.0 tqdm>=4.65 pytest>=7.4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import sys\n",
                "from typing import Any, Callable, List, Tuple, Dict\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import pytorch_lightning as pl\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from itertools import combinations\n",
                "from tqdm import tqdm\n",
                "from transformers import pipeline, AutoTokenizer\n",
                "import json\n",
                "import pickle\n",
                "import random\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# File paths for input CSV files\n",
                "DATA_DIR = \"/kaggle/input/yelp-data\"\n",
                "INPUT_FILES = {\n",
                "    \"business\": os.path.join(DATA_DIR, \"yelp_business_data.csv\"),\n",
                "    \"review\": os.path.join(DATA_DIR, \"yelp_review.csv\"),\n",
                "    \"user\": os.path.join(DATA_DIR, \"yelp_user.csv\"),\n",
                "    \"checkin\": os.path.join(DATA_DIR, \"yelp_checkin_data.csv\"),\n",
                "    \"tip\": os.path.join(DATA_DIR, \"yelp_tip_data.csv\")\n",
                "}\n",
                "\n",
                "# Output paths for processed data\n",
                "OUTPUT_DIR = os.path.join(\"data\", \"processed\")\n",
                "OUTPUT_FILES = {\n",
                "    \"merged_data\": os.path.join(OUTPUT_DIR, \"merged_data.csv\"),\n",
                "    \"featured_data\": os.path.join(OUTPUT_DIR, \"featured_data.csv\"),\n",
                "    \"sentiment_data\": os.path.join(OUTPUT_DIR, \"sentiment_data.csv\"),\n",
                "    \"final_model_data\": os.path.join(OUTPUT_DIR, \"final_model_data.csv\")\n",
                "}\n",
                "\n",
                "FEATURED_DATA_PATH = OUTPUT_FILES[\"featured_data\"]\n",
                "\n",
                "# Model hyperparameters\n",
                "LEARNING_RATE = 0.0001\n",
                "BATCH_SIZE = 64\n",
                "MAX_EPOCHS = 40\n",
                "\n",
                "# Feature lists\n",
                "CANDIDATE_FEATURES = [\n",
                "    \"user_average_stars\",\n",
                "    \"business_average_stars\",\n",
                "    \"user_review_count\",\n",
                "    \"business_review_count\",\n",
                "    \"time_yelping\",\n",
                "    \"date_year\",\n",
                "    \"total_elite_statuses\",\n",
                "    \"elite_status\",\n",
                "    \"normalized_sentiment_score\"\n",
                "]\n",
                "\n",
                "EXPECTED_OPTIMAL_FEATURES = [\n",
                "    \"user_average_stars\",\n",
                "    \"business_average_stars\",\n",
                "    \"time_yelping\",\n",
                "    \"elite_status\",\n",
                "    \"normalized_sentiment_score\"\n",
                "]\n",
                "\n",
                "# Random seed\n",
                "SEED = 1\n",
                "\n",
                "# Sentiment settings\n",
                "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "MAX_TOKENS = 512\n",
                "SENTIMENT_BATCH_SIZE = 64\n",
                "\n",
                "# Device detection helper function\n",
                "def get_device():\n",
                "    \"\"\"\n",
                "    Detect the available device for PyTorch computations.\n",
                "\n",
                "    Returns:\n",
                "        str: 'mps' if MPS is available, 'cuda' if CUDA is available, otherwise 'cpu'\n",
                "    \"\"\"\n",
                "    if torch.backends.mps.is_available():\n",
                "        return \"mps\"\n",
                "    elif torch.cuda.is_available():\n",
                "        return \"cuda\"\n",
                "    else:\n",
                "        return \"cpu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_elite_years(elite_str: str) -> List[int]:\n",
                "    \"\"\"\n",
                "    Parse elite years string into a list of integers.\n",
                "\n",
                "    Handles empty strings, NaN values, and comma/pipe-separated years.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years, e.g., \"2018,2019,2020\" or \"2018|2019\"\n",
                "\n",
                "    Returns:\n",
                "        List of integers representing elite years, or empty list for invalid input\n",
                "    \"\"\"\n",
                "    if pd.isna(elite_str) or elite_str == \"\":\n",
                "        return []\n",
                "\n",
                "    # Replace pipe with comma for consistent splitting\n",
                "    elite_str = elite_str.replace('|', ',')\n",
                "\n",
                "    # Split by comma and convert to integers, filtering out empty strings\n",
                "    years = []\n",
                "    for year_str in elite_str.split(','):\n",
                "        year_str = year_str.strip()\n",
                "        if year_str:\n",
                "            try:\n",
                "                years.append(int(year_str))\n",
                "            except ValueError:\n",
                "                # Skip invalid year strings\n",
                "                continue\n",
                "\n",
                "    return years\n",
                "\n",
                "def count_elite_statuses(elite_str: str, review_year: int) -> int:\n",
                "    \"\"\"\n",
                "    Count the number of elite statuses up to and including the review year.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years\n",
                "        review_year: The year of the review\n",
                "\n",
                "    Returns:\n",
                "        Number of elite years <= review_year\n",
                "    \"\"\"\n",
                "    elite_years = parse_elite_years(elite_str)\n",
                "    return sum(1 for year in elite_years if year <= review_year)\n",
                "\n",
                "def check_elite_status(elite_str: str, review_year: int) -> int:\n",
                "    \"\"\"\n",
                "    Check if the user was elite in the review year or the previous year.\n",
                "\n",
                "    Args:\n",
                "        elite_str: String containing elite years\n",
                "        review_year: The year of the review\n",
                "\n",
                "    Returns:\n",
                "        1 if elite in review_year or (review_year - 1), 0 otherwise\n",
                "    \"\"\"\n",
                "    elite_years = parse_elite_years(elite_str)\n",
                "    return 1 if review_year in elite_years or (review_year - 1) in elite_years else 0\n",
                "\n",
                "def smart_truncate_text(text: str, tokenizer, max_tokens: int = 500) -> str:\n",
                "    \"\"\"\n",
                "    Tokenize text, keep first 250 + last 250 tokens if over max_tokens, convert back to string.\n",
                "    \"\"\"\n",
                "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
                "    if len(tokens) <= max_tokens:\n",
                "        return text\n",
                "    # Keep first 250 and last 250\n",
                "    first_part = tokens[:250]\n",
                "    last_part = tokens[-250:]\n",
                "    truncated_tokens = first_part + last_part\n",
                "    return tokenizer.decode(truncated_tokens)\n",
                "\n",
                "def set_seed(seed: int = 1) -> None:\n",
                "    \"\"\"\n",
                "    Set random seed for reproducibility.\n",
                "    \"\"\"\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.backends.mps.is_available():\n",
                "        torch.mps.manual_seed(seed)\n",
                "\n",
                "def verify_gpu_support() -> bool:\n",
                "    \"\"\"\n",
                "    Check MPS GPU support availability.\n",
                "    \"\"\"\n",
                "    available = torch.backends.mps.is_available()\n",
                "    status = \"available\" if available else \"not available\"\n",
                "    logger.info(f\"MPS GPU support is {status}.\")\n",
                "    return available"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_business_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load business data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the business CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing business data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Business data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for business data\n",
                "    dtypes = {\n",
                "        'business_id': str,\n",
                "        'name': str,\n",
                "        'address': str,\n",
                "        'city': str,\n",
                "        'state': str,\n",
                "        'postal_code': str,\n",
                "        'latitude': float,\n",
                "        'longitude': float,\n",
                "        'stars': float,\n",
                "        'review_count': int,\n",
                "        'is_open': int\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['business_id', 'name', 'stars', 'review_count']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in business data: {missing_columns}\")\n",
                "\n",
                "    return df\n",
                "\n",
                "def load_review_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load review data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the review CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing review data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Review data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for review data\n",
                "    dtypes = {\n",
                "        'review_id': str,\n",
                "        'user_id': str,\n",
                "        'business_id': str,\n",
                "        'stars': int,\n",
                "        'useful': int,\n",
                "        'funny': int,\n",
                "        'cool': int,\n",
                "        'text': str,\n",
                "        'date': str\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in review data: {missing_columns}\")\n",
                "\n",
                "    return df\n",
                "\n",
                "def load_user_data(filepath: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load user data from CSV file with appropriate dtypes.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the user CSV file\n",
                "\n",
                "    Returns:\n",
                "        DataFrame containing user data\n",
                "\n",
                "    Raises:\n",
                "        FileNotFoundError: If the file does not exist\n",
                "        ValueError: If required columns are missing\n",
                "    \"\"\"\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"User data file not found: {filepath}\")\n",
                "\n",
                "    # Define dtypes for user data\n",
                "    dtypes = {\n",
                "        'user_id': str,\n",
                "        'name': str,\n",
                "        'review_count': int,\n",
                "        'yelping_since': str,\n",
                "        'useful': int,\n",
                "        'funny': int,\n",
                "        'cool': int,\n",
                "        'elite': str,\n",
                "        'friends': str,\n",
                "        'fans': int,\n",
                "        'average_stars': float,\n",
                "        'compliment_hot': int,\n",
                "        'compliment_more': int,\n",
                "        'compliment_profile': int,\n",
                "        'compliment_cute': int,\n",
                "        'compliment_list': int,\n",
                "        'compliment_note': int,\n",
                "        'compliment_plain': int,\n",
                "        'compliment_cool': int,\n",
                "        'compliment_funny': int,\n",
                "        'compliment_writer': int,\n",
                "        'compliment_photos': int\n",
                "    }\n",
                "\n",
                "    # Load the data\n",
                "    df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)\n",
                "\n",
                "    # Check for required columns\n",
                "    required_columns = ['user_id', 'name', 'review_count', 'yelping_since', 'average_stars']\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "    if missing_columns:\n",
                "        raise ValueError(f\"Missing required columns in user data: {missing_columns}\")\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rename_columns(user_df: pd.DataFrame, business_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    \"\"\"\n",
                "    Rename columns in user and business DataFrames to avoid naming conflicts.\n",
                "\n",
                "    Args:\n",
                "        user_df: DataFrame containing user data\n",
                "        business_df: DataFrame containing business data\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (renamed_user_df, renamed_business_df)\n",
                "    \"\"\"\n",
                "    # Rename user columns\n",
                "    user_renames = {\n",
                "        'useful': 'total_useful',\n",
                "        'funny': 'total_funny',\n",
                "        'cool': 'total_cool',\n",
                "        'review_count': 'user_review_count',\n",
                "        'name': 'user_name',\n",
                "        'average_stars': 'user_average_stars'\n",
                "    }\n",
                "\n",
                "    # Rename business columns\n",
                "    business_renames = {\n",
                "        'stars': 'business_average_stars',\n",
                "        'review_count': 'business_review_count',\n",
                "        'name': 'business_name'\n",
                "    }\n",
                "\n",
                "    # Apply renames\n",
                "    renamed_user_df = user_df.rename(columns=user_renames)\n",
                "    renamed_business_df = business_df.rename(columns=business_renames)\n",
                "\n",
                "    return renamed_user_df, renamed_business_df\n",
                "\n",
                "def convert_date_columns(review_df: pd.DataFrame, user_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    \"\"\"\n",
                "    Convert date columns to datetime dtype.\n",
                "\n",
                "    Args:\n",
                "        review_df: DataFrame containing review data\n",
                "        user_df: DataFrame containing user data\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (converted_review_df, converted_user_df)\n",
                "    \"\"\"\n",
                "    # Convert 'date' column in review_df to datetime\n",
                "    converted_review_df = review_df.copy()\n",
                "    converted_review_df['date'] = pd.to_datetime(converted_review_df['date'])\n",
                "\n",
                "    # Convert 'yelping_since' column in user_df to datetime\n",
                "    converted_user_df = user_df.copy()\n",
                "    converted_user_df['yelping_since'] = pd.to_datetime(converted_user_df['yelping_since'])\n",
                "\n",
                "    return converted_review_df, converted_user_df\n",
                "\n",
                "def merge_datasets(review_df: pd.DataFrame, user_df: pd.DataFrame, business_df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Merge review, user, and business DataFrames using inner joins.\n",
                "\n",
                "    Args:\n",
                "        review_df: DataFrame containing review data\n",
                "        user_df: DataFrame containing user data\n",
                "        business_df: DataFrame containing business data\n",
                "\n",
                "    Returns:\n",
                "        Merged DataFrame with all three sources combined\n",
                "    \"\"\"\n",
                "    # Inner join review -> user on 'user_id'\n",
                "    merged = review_df.merge(user_df, on='user_id', how='inner')\n",
                "    # Then result -> business on 'business_id'\n",
                "    merged = merged.merge(business_df, on='business_id', how='inner')\n",
                "    return merged\n",
                "\n",
                "def clean_merged_data(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Clean merged DataFrame by removing rows with missing values in critical columns.\n",
                "\n",
                "    Args:\n",
                "        merged_df: Merged DataFrame from merge_datasets\n",
                "\n",
                "    Returns:\n",
                "        Cleaned DataFrame with no missing values in critical columns\n",
                "    \"\"\"\n",
                "    # Drop rows with missing values in specified columns\n",
                "    cleaned = merged_df.dropna(subset=['stars', 'text', 'business_average_stars', 'user_average_stars', 'user_review_count'])\n",
                "    return cleaned\n",
                "\n",
                "def preprocess_pipeline() -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete preprocessing pipeline: load, rename, convert dates, merge, clean, and save.\n",
                "\n",
                "    Returns:\n",
                "        Final preprocessed DataFrame\n",
                "    \"\"\"\n",
                "    # Load all three datasets\n",
                "    review_df = load_review_data(INPUT_FILES[\"review\"])\n",
                "    user_df = load_user_data(INPUT_FILES[\"user\"])\n",
                "    business_df = load_business_data(INPUT_FILES[\"business\"])\n",
                "\n",
                "    # Rename columns\n",
                "    user_df, business_df = rename_columns(user_df, business_df)\n",
                "\n",
                "    # Convert date columns\n",
                "    review_df, user_df = convert_date_columns(review_df, user_df)\n",
                "\n",
                "    # Merge datasets\n",
                "    merged_df = merge_datasets(review_df, user_df, business_df)\n",
                "\n",
                "    # Clean merged data\n",
    "    cleaned_df = clean_merged_data(merged_df)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(OUTPUT_FILES[\"merged_data\"])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    cleaned_df.to_csv(OUTPUT_FILES[\"merged_data\"], index=False)\n",
    "\n",
    "    return cleaned_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def engineer_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Engineer time-based features from the DataFrame.\n",
                "\n",
                "    Calculates time_yelping as the difference between date and yelping_since\n",
                "    in weeks, and extracts date_year from the date column.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with 'date' and 'yelping_since' columns\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added 'time_yelping' and 'date_year' columns\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    # Convert date columns to datetime if they are strings\n",
                "    df['date'] = pd.to_datetime(df['date'])\n",
                "    df['yelping_since'] = pd.to_datetime(df['yelping_since'])\n",
                "\n",
                "    # Calculate time_yelping in weeks\n",
                "    df['time_yelping'] = (df['date'] - df['yelping_since']).dt.total_seconds() / (7 * 24 * 3600)\n",
                "\n",
                "    # Extract year from date\n",
                "    df['date_year'] = df['date'].dt.year\n",
                "\n",
                "    return df\n",
                "\n",
                "def engineer_elite_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Engineer elite status features from the DataFrame.\n",
                "\n",
                "    Creates 'total_elite_statuses' by counting elite years up to the review year,\n",
                "    and 'elite_status' by checking if the user was elite in the review year or previous year.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with 'elite' and 'date_year' columns\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added 'total_elite_statuses' and 'elite_status' columns\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    # Create total_elite_statuses using count_elite_statuses\n",
                "    df['total_elite_statuses'] = df.apply(\n",
                "        lambda row: count_elite_statuses(row['elite'], row['date_year']),\n",
                "        axis=1\n",
                "    )\n",
                "\n",
                "    # Create elite_status using check_elite_status\n",
                "    df['elite_status'] = df.apply(\n",
                "        lambda row: check_elite_status(row['elite'], row['date_year']),\n",
                "        axis=1\n",
                "    )\n",
                "\n",
                "    return df\n",
                "\n",
                "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Handle missing values in the DataFrame.\n",
                "\n",
                "    Fills 'time_yelping' with the median value, and 'total_elite_statuses'\n",
                "    and 'elite_status' with 0.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with missing values handled\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "\n",
                "    df['time_yelping'] = df['time_yelping'].fillna(df['time_yelping'].median())\n",
                "    df['total_elite_statuses'] = df['total_elite_statuses'].fillna(0)\n",
                "    df['elite_status'] = df['elite_status'].fillna(0)\n",
                "\n",
                "    return df\n",
                "\n",
                "def feature_engineering_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete feature engineering pipeline.\n",
                "\n",
                "    Applies time feature engineering, elite feature engineering, handles missing values,\n",
                "    saves the processed data to CSV, and returns the DataFrame.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame\n",
                "\n",
                "    Returns:\n",
                "        Processed DataFrame with engineered features\n",
                "    \"\"\"\n",
                "    df = engineer_time_features(df)\n",
                "    df = engineer_elite_features(df)\n",
                "    df = handle_missing_values(df)\n",
                "\n",
                "    df.to_csv(FEATURED_DATA_PATH, index=False)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def initialize_sentiment_pipeline(device: str = \"cpu\"):\n",
                "    \"\"\"\n",
                "    Initialize sentiment analysis pipeline with device detection.\n",
                "\n",
                "    Args:\n",
                "        device: Device to use ('mps' or 'cpu'). Defaults to 'cpu' for Kaggle compatibility.\n",
                "\n",
                "    Returns:\n",
                "        Hugging Face pipeline for sentiment analysis\n",
                "    \"\"\"\n",
                "    # For Kaggle, use CPU as default\n",
                "    if device not in [\"cpu\", \"cuda\"]:\n",
                "        device = \"cpu\"\n",
                "\n",
                "    # Initialize pipeline\n",
                "    sentiment_pipeline = pipeline(\n",
                "        \"sentiment-analysis\",\n",
                "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "        device=device,\n",
                "        truncation=False\n",
                "    )\n",
                "\n",
                "    return sentiment_pipeline\n",
                "\n",
                "def process_sentiment_batch(texts: List[str], pipeline, batch_size: int = 64) -> List[Dict]:\n",
                "    \"\"\"\n",
                "    Process batch of texts through sentiment analysis pipeline.\n",
                "\n",
                "    Args:\n",
                "        texts: List of text strings to analyze\n",
                "        pipeline: Hugging Face sentiment analysis pipeline\n",
                "        batch_size: Number of texts to process in each batch\n",
                "\n",
                "    Returns:\n",
                "        List of sentiment analysis results (dicts with 'label' and 'score')\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    for i in range(0, len(texts), batch_size):\n",
                "        batch = texts[i:i + batch_size]\n",
                "        batch_results = pipeline(batch)\n",
                "        results.extend(batch_results)\n",
                "    return results\n",
                "\n",
                "def normalize_sentiment_scores(sentiment_results: List[Dict]) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Normalize sentiment scores to range [-1, 1].\n",
                "\n",
                "    Args:\n",
                "        sentiment_results: List of sentiment analysis results\n",
                "\n",
                "    Returns:\n",
                "        Pandas Series with normalized scores (-1 for negative, +1 for positive)\n",
                "    \"\"\"\n",
                "    scores = []\n",
                "    for result in sentiment_results:\n",
                "        label = result['label']\n",
                "        score = result['score']\n",
                "        if label == 'NEGATIVE':\n",
                "            scores.append(-score)\n",
                "        elif label == 'POSITIVE':\n",
                "            scores.append(score)\n",
                "        else:\n",
                "            # Handle unexpected labels (e.g., neutral) by setting to 0\n",
                "            scores.append(0.0)\n",
                "    return pd.Series(scores)\n",
                "\n",
                "def sentiment_analysis_pipeline(df: pd.DataFrame, batch_size: int = 64) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Complete sentiment analysis pipeline.\n",
                "\n",
                "    Args:\n",
                "        df: DataFrame containing review texts in 'text' column\n",
                "        batch_size: Number of texts to process in each batch\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with added sentiment columns\n",
                "    \"\"\"\n",
                "    # Initialize sentiment pipeline\n",
                "    sentiment_pipeline = initialize_sentiment_pipeline()\n",
                "\n",
                "    # Load tokenizer for truncation\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "    # Process in batches with tqdm progress bar\n",
                "    results = []\n",
                "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing sentiment analysis\"):\n",
                "        batch_texts = df['text'].iloc[i:i + batch_size].tolist()\n",
                "        # Apply smart truncation to each text\n",
                "        truncated_texts = [smart_truncate_text(text, tokenizer, max_tokens=500) for text in batch_texts]\n",
                "        batch_results = process_sentiment_batch(truncated_texts, sentiment_pipeline, batch_size)\n",
                "        results.extend(batch_results)\n",
                "\n",
                "    # Normalize sentiment scores\n",
                "    normalized_scores = normalize_sentiment_scores(results)\n",
                "\n",
                "    # Add columns\n",
                "    df['sentiment_label'] = [r['label'] for r in results]\n",
                "    df['sentiment_score_raw'] = [r['score'] for r in results]\n",
                "    df['normalized_sentiment_score'] = normalized_scores\n",
                "\n",
                "    # Save to CSV\n",
                "    output_path = OUTPUT_FILES[\"sentiment_data\"]\n",
                "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "    df.to_csv(output_path, index=False)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_feature_data(df: pd.DataFrame, candidate_features: List[str]) -> Tuple[pd.DataFrame, pd.Series]:\n",
                "    \"\"\"\n",
                "    Prepare data for feature selection.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame containing all features and target\n",
                "        candidate_features: List of feature column names to consider\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (X, y) where X is features DataFrame and y is target Series\n",
                "    \"\"\"\n",
                "    # Select candidate features plus target column\n",
                "    selected_cols = candidate_features + ['stars']\n",
                "    subset_df = df[selected_cols].copy()\n",
                "\n",
                "    # Remove rows with missing values in selected columns\n",
                "    subset_df = subset_df.dropna()\n",
                "\n",
                "    # Separate features and target\n",
                "    X = subset_df[candidate_features]\n",
                "    y = subset_df['stars']\n",
                "\n",
                "    return X, y\n",
                "\n",
                "def run_best_subset_selection(X: pd.DataFrame, y: pd.Series) -> List[str]:\n",
                "    \"\"\"\n",
                "    Run best subset feature selection using exhaustive search over different numbers of features.\n",
                "\n",
                "    Args:\n",
                "        X: Feature DataFrame\n",
                "        y: Target Series\n",
                "\n",
                "    Returns:\n",
                "        List of selected feature names\n",
                "    \"\"\"\n",
                "    # Split data into train and validation\n",
                "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "    best_overall_mse = float('inf')\n",
                "    best_k = None\n",
                "    best_features = None\n",
                "\n",
                "    max_k = min(10, len(X.columns))\n",
                "\n",
                "    for k in tqdm(range(1, max_k + 1), desc=\"Evaluating k\"):\n",
                "        # Get all combinations of k features\n",
                "        feature_combos = list(combinations(X.columns, k))\n",
                "\n",
                "        best_mse_for_k = float('inf')\n",
                "        best_combo_for_k = None\n",
                "\n",
                "        for combo in tqdm(feature_combos, desc=f\"Evaluating combinations for k={k}\"):\n",
                "            combo = list(combo)\n",
                "            # Select features\n",
                "            X_train_combo = X_train[combo]\n",
                "            X_val_combo = X_val[combo]\n",
                "\n",
                "            # Fit model\n",
                "            model = RandomForestRegressor(n_estimators=100, random_state=1, n_jobs=-1)\n",
                "            model.fit(X_train_combo, y_train)\n",
                "\n",
                "            # Predict and compute MSE\n",
                "            y_pred = model.predict(X_val_combo)\n",
                "            mse = mean_squared_error(y_val, y_pred)\n",
                "\n",
                "            if mse < best_mse_for_k:\n",
                "                best_mse_for_k = mse\n",
                "                best_combo_for_k = combo\n",
                "\n",
                "        # Compare across k\n",
                "        if best_mse_for_k < best_overall_mse:\n",
                "            best_overall_mse = best_mse_for_k\n",
                "            best_k = k\n",
                "            best_features = best_combo_for_k\n",
                "\n",
                "    print(f\"Best k: {best_k}\")\n",
                "    print(f\"Best feature set: {best_features}\")\n",
                "    print(f\"Best MSE: {best_overall_mse}\")\n",
                "\n",
                "    return best_features\n",
                "\n",
                "def feature_selection_pipeline(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
                "    \"\"\"\n",
                "    Complete feature selection pipeline.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame with all features and target\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (final DataFrame with optimal features + target, list of optimal features)\n",
                "    \"\"\"\n",
                "    # Get candidate features from config\n",
                "    candidate_features = CANDIDATE_FEATURES\n",
                "\n",
                "    # Prepare feature data\n",
                "    X, y = prepare_feature_data(df, candidate_features)\n",
                "\n",
                "    # Run best subset selection\n",
                "    optimal_features = run_best_subset_selection(X, y)\n",
                "\n",
                "    # Save optimal features to JSON\n",
                "    optimal_features_path = os.path.join(OUTPUT_DIR, \"optimal_features.json\")\n",
                "    with open(optimal_features_path, 'w') as f:\n",
                "        json.dump(optimal_features, f, indent=2)\n",
                "\n",
                "    # Create final dataset with optimal features + target\n",
                "    final_cols = optimal_features + ['stars']\n",
                "    final_df = df[final_cols].copy()\n",
                "\n",
                "    # Remove any remaining missing values\n",
                "    final_df = final_df.dropna()\n",
                "\n",
                "    # Save final dataset\n",
    "    final_data_path = OUTPUT_FILES[\"final_model_data\"]\n",
    "    os.makedirs(os.path.dirname(final_data_path), exist_ok=True)\n",
    "    final_df.to_csv(final_data_path, index=False)\n",
    "\n",
    "    # Return final DataFrame and feature list\n",
    "    return final_df, optimal_features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class YelpRatingPredictor(pl.LightningModule):\n",
                "    \"\"\"\n",
                "    PyTorch Lightning module for predicting Yelp ratings using a neural network.\n",
                "\n",
                "    This model consists of a feedforward neural network with dropout and batch normalization\n",
                "    layers, designed to predict star ratings based on input features.\n",
                "\n",
                "    Attributes:\n",
                "        network: Sequential neural network layers\n",
                "        criterion: Mean squared error loss function\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, input_size: int = 5, learning_rate: float = 0.0001) -> None:\n",
                "        super().__init__()\n",
                "        self.network = nn.Sequential(\n",
                "            nn.Linear(input_size, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "        self.criterion = nn.MSELoss()\n",
                "        self.save_hyperparameters()\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the network.\n",
                "\n",
                "        Args:\n",
                "            x: Input tensor\n",
                "\n",
                "        Returns:\n",
                "            Output tensor predictions\n",
                "        \"\"\"\n",
                "        return self.network(x)\n",
                "\n",
                "    def training_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Training step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Loss tensor\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        self.log('train_loss', loss)\n",
                "        return loss\n",
                "\n",
                "    def validation_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Validation step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Loss tensor\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        mae = torch.mean(torch.abs(preds - y))\n",
                "        self.log('val_loss', loss)\n",
                "        self.log('val_mae', mae)\n",
                "        return loss\n",
                "\n",
                "    def test_step(self, batch: tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\n",
                "        \"\"\"\n",
                "        Test step for one batch.\n",
                "\n",
                "        Args:\n",
                "            batch: Tuple of (features, targets)\n",
                "            batch_idx: Batch index\n",
                "\n",
                "        Returns:\n",
                "            Dictionary with test loss and MAE\n",
                "        \"\"\"\n",
                "        x, y = batch\n",
                "        preds = self(x)\n",
                "        loss = self.criterion(preds, y)\n",
                "        mae = torch.mean(torch.abs(preds - y))\n",
                "        return {'test_loss': loss, 'test_mae': mae}\n",
                "\n",
                "    def configure_optimizers(self) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Configure optimizer and learning rate scheduler.\n",
                "\n",
                "        Returns:\n",
                "            Dictionary with optimizer and scheduler configuration\n",
                "        \"\"\"\n",
                "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams.learning_rate)\n",
                "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "            optimizer, factor=0.5, patience=3\n",
                "        )\n",
                "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}}"
            ]
        },
                "def stratify_and_split(df: pd.DataFrame, target_size: int = 130000) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stratify the dataset by 'stars' and downsample to equal samples per class.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame containing 'stars' column\n",
                "        target_size: Total target size for the stratified dataset\n",
                "\n",
                "    Returns:\n",
                "        Stratified DataFrame with equal samples per class\n",
                "    \"\"\"\n",
                "    # Group by 'stars' and downsample each group\n",
                "    samples_per_class = target_size // 5  # 5 classes (1-5 stars)\n",
                "\n",
                "    stratified_df = df.groupby('stars', group_keys=False).apply(\n",
                "        lambda x: x.sample(n=min(len(x), samples_per_class), random_state=1)\n",
                "    ).reset_index(drop=True)\n",
                "\n",
                "    return stratified_df\n",
                "\n",
                "def prepare_train_test_data(df: pd.DataFrame, features: List[str], test_size: float = 0.2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, MinMaxScaler]:\n",
                "    \"\"\"\n",
                "    Prepare train/test data with stratification, normalization, and PyTorch tensor conversion.\n",
                "\n",
                "    Args:\n",
                "        df: Input DataFrame\n",
                "        features: List of feature column names\n",
                "        test_size: Fraction of data to use for testing\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (X_train, X_test, y_train, y_test, scaler)\n",
                "    \"\"\"\n",
                "    # Prepare features and target\n",
                "    X = df[features]\n",
                "    y = df['stars']\n",
                "\n",
                "    # Split into train/test with stratification\n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X, y, test_size=test_size, stratify=y, random_state=1\n",
                "    )\n",
                "\n",
                "    # Normalize features using MinMaxScaler\n",
                "    scaler = MinMaxScaler()\n",
                "    X_train_scaled = scaler.fit_transform(X_train)\n",
                "    X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "    # Convert to PyTorch tensors\n",
                "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
                "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
                "    y_train_tensor = torch.FloatTensor(y_train.values)\n",
                "    y_test_tensor = torch.FloatTensor(y_test.values)\n",
                "\n",
                "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, scaler\n",
                "\n",
                "def create_dataloaders(X_train, y_train, X_val, y_val, batch_size: int = 64) -> Tuple[DataLoader, DataLoader]:\n",
                "    \"\"\"\n",
                "    Create DataLoaders for training and validation datasets.\n",
                "\n",
                "    Args:\n",
                "        X_train: Training features tensor\n",
                "        y_train: Training labels tensor\n",
                "        X_val: Validation features tensor\n",
                "        y_val: Validation labels tensor\n",
                "        batch_size: Batch size for DataLoaders\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (train_loader, val_loader)\n",
                "    \"\"\"\n",
                "    train_dataset = TensorDataset(X_train, y_train)\n",
                "    val_dataset = TensorDataset(X_val, y_val)\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
                "    return train_loader, val_loader\n",
                "\n",
                "def train_model(model, train_loader, val_loader, max_epochs: int = 40) -> pl.Trainer:\n",
                "    \"\"\"\n",
                "    Train the model using PyTorch Lightning.\n",
                "\n",
                "    Args:\n",
                "        model: PyTorch Lightning model to train\n",
                "        train_loader: Training DataLoader\n",
                "        val_loader: Validation DataLoader\n",
                "        max_epochs: Maximum number of epochs\n",
                "\n",
                "    Returns:\n",
                "        Trained PyTorch Lightning Trainer\n",
                "    \"\"\"\n",
                "    # Detect device\n",
                "    accelerator = 'cpu'  # For Kaggle compatibility\n",
                "\n",
                "    # Configure callbacks\n",
                "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
                "\n",
                "    # Configure trainer\n",
                "    trainer = pl.Trainer(\n",
                "        accelerator=accelerator,\n",
                "        max_epochs=max_epochs,\n",
                "        callbacks=[early_stopping]\n",
                "    )\n",
                "\n",
                "    # Train model\n",
                "    trainer.fit(model, train_loader, val_loader)\n",
                "\n",
                "    # Save model\n",
                "    os.makedirs('models', exist_ok=True)\n",
                "    torch.save(model.state_dict(), os.path.join('models', 'best_model.pt'))\n",
                "\n",
                "    return trainer\n",
                "\n",
                "def evaluate_model(model, X_test, y_test) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Evaluate the model on test data and return metrics.\n",
                "\n",
                "    Args:\n",
                "        model: Trained PyTorch model\n",
                "        X_test: Test features tensor\n",
                "        y_test: Test labels tensor\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with MSE, MAE, and R metrics\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        predictions = model(X_test)\n",
                "        mse = mean_squared_error(y_test.cpu().numpy(), predictions.cpu().numpy())\n",
                "        mae = mean_absolute_error(y_test.cpu().numpy(), predictions.cpu().numpy())\n",
                "        r2 = r2_score(y_test.cpu().numpy(), predictions.cpu().numpy())\n",
                "    return {'mse': mse, 'mae': mae, 'r2': r2}\n",
                "\n",
                "def training_pipeline() -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Complete training pipeline: load data, train model, evaluate, and save artifacts.\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with results including metrics and file paths\n",
                "    \"\"\"\n",
                "    # Load final model data and optimal features\n",
                "    df = pd.read_csv('data/processed/final_model_data.csv')\n",
                "    with open('data/processed/optimal_features.json', 'r') as f:\n",
                "        features = json.load(f)\n",
                "\n",
                "    # Stratify data\n",
                "    df_stratified = stratify_and_split(df)\n",
                "\n",
                "    # Prepare train/test splits\n",
                "    X_train, X_test, y_train, y_test, scaler = prepare_train_test_data(df_stratified, features)\n",
                "\n",
                "    # Split train into train/val for training\n",
                "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
                "        X_train, y_train, test_size=0.2, random_state=1\n",
                "    )\n",
                "\n",
                "    # Create DataLoaders\n",
                "    train_loader, val_loader = create_dataloaders(\n",
                "        X_train_split, y_train_split, X_val, y_val, batch_size=BATCH_SIZE\n",
                "    )\n",
                "\n",
                "    # Initialize model\n",
                "    input_size = len(features)\n",
                "    model = YelpRatingPredictor(input_size=input_size, learning_rate=LEARNING_RATE)\n",
                "\n",
                "    # Train model\n",
                "    trainer = train_model(model, train_loader, val_loader, max_epochs=MAX_EPOCHS)\n",
                "\n",
                "    # Evaluate on test set\n",
                "    metrics = evaluate_model(model, X_test, y_test)\n",
                "\n",
                "    # Save scaler\n",
                "    os.makedirs('models', exist_ok=True)\n",
                "    with open('models/scaler.pkl', 'wb') as f:\n",
                "        pickle.dump(scaler, f)\n",
                "\n",
                "    # Save metrics\n",
                "    os.makedirs('outputs', exist_ok=True)\n",
                "    with open('outputs/metrics.json', 'w') as f:\n",
                "        json.dump(metrics, f)\n",
                "\n",
                "    # Return results\n",
                "    return {\n",
                "        'metrics': metrics,\n",
                "        'model_path': 'models/best_model.pt',\n",
                "        'scaler_path': 'models/scaler.pkl',\n",
                "        'metrics_path': 'outputs/metrics.json'\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def check_imports():\n",
                "    \"\"\"Check if all required packages can be imported.\"\"\"\n",
                "    print(\"Checking package imports...\")\n",
                "    try:\n",
                "        import torch\n",
                "        import transformers\n",
                "        import pandas\n",
                "        import numpy\n",
                "        import sklearn\n",
                "        import pytorch_lightning\n",
                "        import pyarrow\n",
                "        import dotenv\n",
                "        import tqdm\n",
                "        import pytest\n",
                "        print(\" All required packages imported successfully\")\n",
                "        return True\n",
                "    except ImportError as e:\n",
                "        print(f\" Import error: {e}\")\n",
                "        return False\n",
                "\n",
                "def check_pytorch():\n",
                "    \"\"\"Check PyTorch installation and GPU support.\"\"\"\n",
                "    print(\"\\nChecking PyTorch configuration...\")\n",
                "    try:\n",
                "        import torch\n",
                "        print(f\"  PyTorch version: {torch.__version__}\")\n",
                "        print(f\"  MPS available: {torch.backends.mps.is_available()}\")\n",
                "        print(f\"  MPS built: {torch.backends.mps.is_built()}\")\n",
                "        if torch.backends.mps.is_available():\n",
                "            print(\" Apple GPU (MPS) support is available\")\n",
                "        else:\n",
                "            print(\" Apple GPU (MPS) support is not available (will use CPU)\")\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(f\" PyTorch check failed: {e}\")\n",
                "        return False\n",
                "\n",
                "def check_project_structure():\n",
                "    \"\"\"Check if required directories and files exist.\"\"\"\n",
                "    print(\"\\nChecking project structure...\")\n",
                "    required_dirs = ['data', 'models', 'outputs']\n",
                "    required_files = [\n",
                "        INPUT_FILES[\"business\"],\n",
                "        INPUT_FILES[\"review\"],\n",
                "        INPUT_FILES[\"user\"]\n",
                "    ]\n",
                "    \n",
                "    all_good = True\n",
                "    for dir_name in required_dirs:\n",
                "        if os.path.isdir(dir_name):\n",
                "            print(f\"   Directory '{dir_name}' exists\")\n",
                "        else:\n",
                "            print(f\"   Directory '{dir_name}' will be created\")\n",
                "    \n",
                "    for file_name in required_files:\n",
                "        if os.path.isfile(file_name):\n",
                "            print(f\"   File '{file_name}' exists\")\n",
                "        else:\n",
                "            print(f\"   File '{file_name}' is missing\")\n",
                "            all_good = False\n",
                "    \n",
                "    return all_good\n",
                "\n",
                "def check_source_modules():\n",
                "    \"\"\"Check if source modules can be imported.\"\"\"\n",
                "    print(\"\\nChecking source modules...\")\n",
                "    # Since all code is inline, this is always true\n",
                "    print(\" All source modules are inline\")\n",
                "    return True\n",
                "\n",
                "def check_config():\n",
                "    \"\"\"Check configuration settings.\"\"\"\n",
                "    print(\"\\nChecking configuration...\")\n",
                "    try:\n",
                "        print(f\"  Data directory: {DATA_DIR}\")\n",
                "        print(f\"  Learning rate: {LEARNING_RATE}\")\n",
                "        print(f\"  Batch size: {BATCH_SIZE}\")\n",
                "        print(f\"  Device: {get_device()}\")\n",
                "        print(\" Configuration loaded successfully\")\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(f\" Configuration check failed: {e}\")\n",
                "        return False\n",
                "\n",
                "def main_verify():\n",
                "    \"\"\"Run all environment checks.\"\"\"\n",
                "    print(\"=\" * 60)\n",
                "    print(\"EC349 Project Environment Verification\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    checks = [\n",
                "        check_imports(),\n",
                "        check_pytorch(),\n",
                "        check_project_structure(),\n",
                "        check_source_modules(),\n",
                "        check_config()\n",
                "    ]\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    if all(checks):\n",
                "        print(\" Environment is ready!\")\n",
                "        print(\"\\nYou can now run the project pipeline.\")\n",
                "        return 0\n",
                "    else:\n",
                "        print(\" Some checks failed. Please review the output above.\")\n",
                "        return 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_stage(stage_name: str, stage_func: Callable[[], Any]) -> Any:\n",
                "    \"\"\"\n",
                "    Wrapper function to run a pipeline stage with error handling and logging.\n",
                "\n",
                "    Args:\n",
                "        stage_name: Name of the pipeline stage for logging\n",
                "        stage_func: Function to execute for this stage\n",
                "\n",
                "    Returns:\n",
                "        Result of the stage function if successful\n",
                "\n",
                "    Raises:\n",
                "        SystemExit: If a critical error occurs (FileNotFoundError or general Exception)\n",
                "    \"\"\"\n",
                "    print(f\"Starting stage: {stage_name}\")\n",
                "    try:\n",
                "        result = stage_func()\n",
                "        print(f\"Completed stage: {stage_name}\")\n",
                "        return result\n",
                "    except FileNotFoundError as e:\n",
                "        print(f\"File not found in stage '{stage_name}': {e}\")\n",
                "        print(\"Please ensure all required data files are present in the data/ directory.\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"Error in stage '{stage_name}': {e}\")\n",
                "        print(\"Pipeline execution failed. Check logs for details.\")\n",
                "        raise\n",
                "\n",
                "def data_loading_preprocessing_stage() -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stage 1: Data Loading & Preprocessing\n",
                "\n",
                "    Loads raw Yelp datasets, performs preprocessing including column renaming,\n",
                "    date conversion, dataset merging, and data cleaning. Saves the merged data.\n",
                "\n",
                "    Returns:\n",
                "        Preprocessed DataFrame ready for feature engineering\n",
                "    \"\"\"\n",
                "    print(\"Loading and preprocessing raw data...\")\n",
                "    df = preprocess_pipeline()\n",
                "    print(f\"Preprocessed data shape: {df.shape}\")\n",
                "    return df\n",
                "\n",
                "def feature_engineering_stage(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stage 2: Feature Engineering\n",
                "\n",
                "    Creates time-based and elite status features, handles missing values,\n",
                "    and saves the featured data.\n",
                "\n",
                "    Args:\n",
                "        df: Preprocessed DataFrame from stage 1\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with engineered features\n",
                "    \"\"\"\n",
                "    print(\"Engineering features...\")\n",
                "    df_featured = feature_engineering_pipeline(df)\n",
                "    print(f\"Featured data shape: {df_featured.shape}\")\n",
                "    return df_featured\n",
                "\n",
                "def sentiment_analysis_stage(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stage 3: Sentiment Analysis\n",
                "\n",
                "    Performs sentiment analysis on review texts using a pre-trained transformer model,\n",
                "    normalizes sentiment scores, and adds sentiment features to the DataFrame.\n",
                "\n",
                "    Args:\n",
                "        df: Featured DataFrame from stage 2\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with sentiment analysis features\n",
                "    \"\"\"\n",
                "    print(\"Performing sentiment analysis...\")\n",
                "    df_sentiment = sentiment_analysis_pipeline(df)\n",
                "    print(f\"Sentiment data shape: {df_sentiment.shape}\")\n",
                "    return df_sentiment\n",
                "\n",
                "def feature_selection_stage(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
                "    \"\"\"\n",
                "    Stage 4: Feature Selection\n",
                "\n",
                "    Uses best subset selection to identify optimal features for model training,\n",
                "    saves the selected features and final dataset.\n",
                "\n",
                "    Args:\n",
                "        df: DataFrame with all features from stage 3\n",
                "\n",
                "    Returns:\n",
                "        Tuple of (final DataFrame with optimal features, list of optimal features)\n",
                "    \"\"\"\n",
                "    print(\"Performing feature selection...\")\n",
                "    final_df, optimal_features = feature_selection_pipeline(df)\n",
                "    print(f\"Selected {len(optimal_features)} optimal features: {optimal_features}\")\n",
                "    print(f\"Final model data shape: {final_df.shape}\")\n",
                "    return final_df, optimal_features\n",
                "\n",
                "def model_training_stage() -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Stage 5: Model Training\n",
                "\n",
                "    Trains a neural network model using the selected features, evaluates on test data,\n",
                "    and saves the trained model, scaler, and metrics.\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with training results including metrics and file paths\n",
                "    \"\"\"\n",
                "    print(\"Training the model...\")\n",
                "    results = training_pipeline()\n",
                "    print(f\"Training completed. Metrics: {results['metrics']}\")\n",
                "    return results\n",
                "\n",
                "def output_generation_inference_stage() -> None:\n",
                "    \"\"\"\n",
                "    Stage 6: Output Generation/Inference\n",
                "\n",
                "    Loads the trained model and generates example predictions to demonstrate\n",
                "    the inference pipeline. Saves prediction results to outputs/predictions.json.\n",
                "    \"\"\"\n",
                "    print(\"Generating outputs and running inference demo...\")\n",
                "\n",
                "    # File paths\n",
                "    model_path = 'models/best_model.pt'\n",
                "    scaler_path = 'models/scaler.pkl'\n",
                "    features_path = 'data/processed/optimal_features.json'\n",
                "    output_path = 'outputs/predictions.json'\n",
                "\n",
                "    # Check if required files exist\n",
                "    for path in [model_path, scaler_path, features_path]:\n",
                "        if not os.path.exists(path):\n",
                "            raise FileNotFoundError(f\"Required file not found: {path}\")\n",
                "\n",
                "    # Load model, scaler, and features\n",
                "    with open(features_path, 'r') as f:\n",
                "        optimal_features = json.load(f)\n",
                "\n",
                "    input_size = len(optimal_features)\n",
                "    model = YelpRatingPredictor(input_size=input_size)\n",
                "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
                "    model.eval()\n",
                "\n",
                "    with open(scaler_path, 'rb') as f:\n",
                "        scaler = pickle.load(f)\n",
                "\n",
                "    # Create example data for inference\n",
                "    example_data = {\n",
                "        'user_average_stars': [4.2, 3.8, 4.5],\n",
                "        'business_average_stars': [4.0, 3.5, 4.8],\n",
                "        'time_yelping': [52.3, 24.1, 156.7],\n",
                "        'elite_status': [1, 0, 1],\n",
                "        'normalized_sentiment_score': [0.8, -0.3, 0.9]\n",
                "    }\n",
                "\n",
                "    # Filter to optimal features\n",
                "    filtered_data = {k: v for k, v in example_data.items() if k in optimal_features}\n",
                "    df = pd.DataFrame(filtered_data)\n",
                "\n",
                "    # Make predictions\n",
                "    scaled_data = scaler.transform(df.values)\n",
                "    input_tensor = torch.FloatTensor(scaled_data)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        predictions = model(input_tensor).flatten().numpy()\n",
                "\n",
                "    # Save predictions\n",
                "    os.makedirs('outputs', exist_ok=True)\n",
                "    prediction_results = {\n",
                "        'predictions': predictions.tolist(),\n",
                "        'input_features': optimal_features,\n",
                "        'example_inputs': df.to_dict('records')\n",
                "    }\n",
                "\n",
                "    with open(output_path, 'w') as f:\n",
                "        json.dump(prediction_results, f, indent=2)\n",
                "\n",
                "    print(f\"Inference completed. Predictions saved to {output_path}\")\n",
                "    print(f\"Example predictions: {predictions}\")\n",
                "\n",
                "def main_pipeline():\n",
                "    \"\"\"\n",
                "    Main function to execute the complete ML pipeline.\n",
                "\n",
                "    Runs all pipeline stages sequentially with progress tracking and error handling.\n",
                "    \"\"\"\n",
                "    print(\"Starting Automated ML Pipeline for Yelp Rating Prediction\")\n",
                "\n",
                "    # Execute pipeline stages sequentially\n",
                "    stages = [\n",
                "        (\"Data Loading & Preprocessing\", data_loading_preprocessing_stage),\n",
                "        (\"Feature Engineering\", lambda: feature_engineering_stage(df)),\n",
                "        (\"Sentiment Analysis\", lambda: sentiment_analysis_stage(df)),\n",
                "        (\"Feature Selection\", lambda: feature_selection_stage(df)),\n",
                "        (\"Model Training\", model_training_stage),\n",
                "        (\"Output Generation/Inference\", output_generation_inference_stage)\n",
                "    ]\n",
                "\n",
                "    df = None\n",
                "    for stage_name, stage_func in stages:\n",
                "        if stage_name == \"Data Loading & Preprocessing\":\n",
                "            df = run_stage(stage_name, stage_func)\n",
                "        elif stage_name in [\"Feature Engineering\", \"Sentiment Analysis\", \"Feature Selection\"]:\n",
                "            df = run_stage(stage_name, stage_func)\n",
                "        else:\n",
                "            run_stage(stage_name, stage_func)\n",
                "\n",
                "    print(\"Pipeline execution completed successfully!\")\n",
                "    print(\"Check the outputs/ directory for results.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('integrated_yelp_notebook.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stratify_and_split(df: pd.DataFrame, target_size: int = 130000) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Stratify the dataset by 'stars' and downsample to equal samples per class.\n",
                "\n",
